{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a flexible class of supervised algorithms for both classification and regression. As we discussed in class there are an infinite number of linear discriminative classifiers that could separate two sets of data, and thereby create a model for classification.\n",
    "Such as for the two-dimensional data like that shown here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
    "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "Evidently our simple intuition of \"drawing a line between classes\" is not good enough, and we need to think a bit more deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines: Maximizing the Margin\n",
    "\n",
    "Support vector machines offer one way to improve on this.\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='lightgray', alpha=0.5)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line that maximizes this margin is the one we will choose as the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Support Vector Machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier (`SVC`) to train an SVM model on this data.\n",
    "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, edgecolors='black',\n",
    "                   facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are circled in the following figure.\n",
    "These points are the pivotal elements of this fit; they are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identities of these points are stored in the `support_vectors_` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to this classifier's success is that for the fit, only the positions of the support vectors matter; any points further from the margin that are on the correct side do not modify the fit.\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for 60 training points.\n",
    "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors in the left panel are the same as the support vectors in the right panel.\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Linear Boundaries: Kernel SVM\n",
    "\n",
    "Where SVM can become quite powerful is when it is combined with *kernels*. To motivate the need for kernels, let's look at some data that is not linearly separable (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data.  However, one simple projection we could use would be to compute a *radial basis function* (RBF) centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this extra data dimension using a three-dimensional plot, as seen in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "ax.view_init(elev=20, azim=30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.\n",
    "\n",
    "In this case we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection.\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF kernel, using the `kernel` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our previously defined function to visualize the fit and identify the support vectors (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap?\n",
    "For example, you may have data like this (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge factor that \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as `C`.\n",
    "For a very large `C`, the margin is hard, and points cannot lie in it.\n",
    "For a smaller `C`, the margin is softer and can grow to encompass some points.\n",
    "\n",
    "The plot shown in the following figure gives a visual picture of how a changing `C` affects the final fit via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of `C` will depend on your dataset, and you should tune this parameter using cross-validation or a similar procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST is a dataset developed by Zalando, an online fashion retailer, as a more challenging alternative to the original MNIST dataset which consists of handwritten digits. Fashion MNIST comprises 70,000 grayscale images across 10 different categories of fashion items. These categories include T-shirts/tops, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots. Each image is 28x28 pixels, providing a standardized frame for straightforward input into machine learning models.\r\n",
    "\r\n",
    "The primary purpose of Fashion MNIST is to serve as a benchmark for machine learning algorithms, particularly in the realm of image recognition and classification. Its creation was motivated by the need for a more difficult dataset than MNIST, which had become too easy for modern machine learning techniques, diminishing its usefulness as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "fashion_mnist = fetch_openml(name='Fashion-MNIST', version=1)\n",
    "\n",
    "# The data key contains the images, and the target key contains the labels\n",
    "images, labels = fashion_mnist.data, fashion_mnist.target\n",
    "\n",
    "# Convert images from flat (1D) arrays to 2D arrays (28x28 pixels)\n",
    "images = np.array(images).reshape(-1, 28, 28).astype('float32')\n",
    "\n",
    "# Normalize the images to have values between 0 and 1\n",
    "images /= 255.0\n",
    "\n",
    "# Convert labels to integers\n",
    "labels = labels.astype('int64')\n",
    "\n",
    "# Plot the first 25 images from the dataset\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4-1:** SVM Classifier for Fashion MNIST\n",
    "\n",
    "In this exercise, you will develop a machine learning model using a Support Vector Machine (SVM) to classify images from the Fashion MNIST dataset. This will involve several steps, from loading and preprocessing the data to training and evaluating your SVM model. The goal is to provide practical experience with SVM for image classification tasks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "SVMs require input data in a specific format and are not inherently designed for multiclass classification. Scikit-learn addresses this by employing one-vs-one or one-vs-all strategies for multiclass classification. You'll need to preprocess the images by flattening them from a 2D 28x28 pixel matrix to a 1D array of 784 features, ensuring the labels are also prepared correctly for training.\n",
    "\n",
    "### Steps and Functions to Implement\n",
    "\n",
    "- **Data Preprocessing**\n",
    "  - Flatten the images to convert them from a 2D array (28x28 pixels) to a 1D array (784 features) to fit the input requirements of SVMs.\n",
    "  \n",
    "    **Function to Create:**\n",
    "    - `flatten_images(images)`: Converts images from a 2D array to a 1D array.\n",
    "\n",
    "- **Model Training**\n",
    "  - Train an SVM model on the preprocessed training data. Begin with a linear kernel and the default regularization parameter (C). You are encouraged to experiment with other kernels (e.g., RBF) and parameters as part of the exercise.\n",
    "\n",
    "- **Model Evaluation**\n",
    "  - Evaluate the performance of your SVM model on the test dataset. Calculate and report metrics such as accuracy, precision, recall, and F1-score to gauge the effectiveness of your model.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the process of preparing data for SVM classification.\n",
    "- Gain experience in training and evaluating SVM models.\n",
    "- Explore the impact of different kernels and hyperparameters on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Decision Trees\r\n",
    "\r\n",
    "Regression Decision Trees are specialized decision trees designed for regression tasks, where the objective is to predict a continuous outcome. Unlike classification trees that predict a class label, regression trees predict a numerical quantity.\r\n",
    "\r\n",
    "## How Regression Decision Trees Work\r\n",
    "\r\n",
    "1. **Splitting Criteria**: The dataset is divided based on feature values. Splits are chosen to minimize variance within each node, aiming for nodes with homogeneous target values. This ideally reduces variance and improves prediction accuracy.\r\n",
    "\r\n",
    "2. **Tree Construction**: \r\n",
    "   - Starting from the root node, the entire dataset is considered.\r\n",
    "   - The best split is determined by evaluating each feature to find the split that results in the greatest reduction in variance (or sometimes mean squared error).\r\n",
    "   - This process is recursively applied to each child node until stopping criteria are met.\r\n",
    "\r\n",
    "3. **Stopping Criteria**: \r\n",
    "   - To prevent overfitting, the tree's growth is controlled by criteria such as maximum depth, minimum number of samples per node, and minimum improvement in variance reduction for a split.\r\n",
    "\r\n",
    "4. **Prediction**: \r\n",
    "   - For a new data point, predictions are made by traversing the tree based on its feature values until reaching a leaf node.\r\n",
    "   - The prediction is the mean target value of the samples in the leaf node.\r\n",
    "\r\n",
    "## Advantages\r\n",
    "\r\n",
    "- **Interpretability**: Easy to understand and interpret, making them useful in scenarios where decision-making processes need to be transparent.\r\n",
    "- **Non-linearity**: Capable of capturing non-linear relationships without requiring data transformation.\r\n",
    "- **Feature Interactions**: Automatically model interactions between features.\r\n",
    "\r\n",
    "## Disadvantages\r\n",
    "\r\n",
    "- **Overfitting**: Can grow complex and overfit the training data, capturing noise rather than the underlying pattern.\r\n",
    "- **Variance**: Small changes in the data can result in different trees, leading to high variance. Ensemble methods like Random Forests can help reduce this.\r\n",
    "- **Bias**: A single tree might be too simple to capture complex relationships, potentially leading to underfitting. This issue can be addressed by using deeper trees or ensemble methods.\r\n",
    "\r\n",
    "Regression Decision Trees balance simplicity and flexibility, making them a compelling choice for regression tasks. However, their propensity for overfitting and high variance often leads to their use as components of more complex ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we looked at the Boston Housing data and constructed a linear regression model.  In the following exercise, we will construct a regression tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "boston=pd.read_csv('../data/BostonHousing.csv')\n",
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIM - per capita crime rate by town <br>\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft. <br>\n",
    "INDUS - proportion of non-retail business acres per town. <br>\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise) <br>\n",
    "NOX - nitric oxides concentration (parts per 10 million) <br>\n",
    "RM - average number of rooms per dwelling <br>\n",
    "AGE - proportion of owner-occupied units built prior to 1940 <br>\n",
    "DIS - weighted distances to five Boston employment centres <br>\n",
    "RAD - index of accessibility to radial highways <br>\n",
    "TAX - full-value property-tax rate per 10,000 <br>\n",
    "PTRATIO - pupil-teacher ratio by town <br>\n",
    "MEDV - Median value of owner-occupied homes in 1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston[boston.columns[:-1]]\n",
    "y = boston[['medv']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4-2:** Construct the Regression Tree using the Boston Housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** Which feature was most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4-3:** Given the below table, classify the test sample X = (age = youth, income = low, student = yes, credit_rating = fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/nv_bayes.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
